---
title: Azure Data Factory の Mapping Data Flow 機能のシンク変換を設定する
description: Mapping Data Flow のシンク変換を設定する方法について説明します。
author: kromerm
ms.author: makromer
ms.service: data-factory
ms.topic: conceptual
ms.date: 02/03/2019
ms.openlocfilehash: 24ad0f2e917420c327577851cabc9e5bdbad2825
ms.sourcegitcommit: 0e59368513a495af0a93a5b8855fd65ef1c44aac
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/15/2019
ms.locfileid: "69515684"
---
# <a name="sink-transformation-for-a-data-flow"></a>データ フローのシンク変換

[!INCLUDE [notes](../../includes/data-factory-data-flow-preview.md)]

データ フローを変換した後、データを変換先のデータセットにシンクできます。 シンク変換では、変換先の出力データのデータセット定義を選択します。 データ フローに必要な数だけシンク変換を設定できます。

スキーマの誤差や受信データの変更に対処するには、出力データセットに定義済みのスキーマがない状態で出力データをフォルダーにシンクします。 ソースで **[Allow schema drift]** (スキーマの誤差を許可) を選択することで、ソース内の列変更に対処することもできます。 この場合、シンク内のすべてのフィールドが自動マッピングされます。

![[Auto Map]\(自動マップ) オプションを含む [シンク] タブのオプション](media/data-flow/sink1.png "シンク 1")

すべての受信フィールドをシンクするには、 **[Auto Map]** (自動マップ) をオンにします。 変換先にシンクするフィールドを選択したり、変換先でのフィールドの名前を変更するには、 **[Auto Map]** (自動マップ) をオフにします。 続いて **[マッピング]** タブを開いて、出力フィールドをマップします。

![[マッピング] タブのオプション](media/data-flow/sink2.png "シンク 2")

## <a name="output"></a>Output 
Azure BLOB ストレージまたは Data Lake Storage のシンクの種類の場合、変換されたデータをフォルダーに出力します。 Spark は、シンク変換で使用されるパーティション分割スキームに基づいて、パーティション分割された出力データ ファイルを生成します。 

このパーティション分割スキームは、 **[最適化]** タブから設定できます。Data Factory で 1 つのファイルに出力をマージさせる場合は、 **[単一パーティション]** を選択します。

![[最適化] タブのオプション](media/data-flow/opt001.png "シンク オプション")

## <a name="field-mapping"></a>フィールドのマッピング
シンク変換の **[マッピング]** タブでは、左側の受信の列を右側の変換先にマップできます。 データ フローをファイルにシンクした場合、Data Factory は常に、新しいファイルをフォルダーに書き込みます。 データベース データセットにマップする場合は、挿入、更新、upsert、または削除するデータベース テーブル操作のオプションを選択します。

![[マッピング] タブ](media/data-flow/sink2.png "シンク")

マッピング テーブルでは、複数選択を行って、複数の列をリンクしたり、複数の列のリンクを削除したり、複数の行を同じ列名にマップしたりできます。

常に受信フィールド セットをターゲットにマップしておき、柔軟なスキーマ定義を完全に受け入れるには、 **[Allow schema drift]** (スキーマの誤差を許可) を選択します。

![[マッピング] タブ、データセット内の列にマップされたフィールドを表示](media/data-flow/multi1.png "複数のオプション")

列マッピングをリセットするには、 **[Re-map]** (再マップ) を選択します。

![[シンク] タブ](media/data-flow/sink1.png "シンク 1")

スキーマが変更された場合にシンクを失敗させるには、 **[スキーマの検証]** を選択します。

変換先ファイルをターゲット フォルダーに書き込む前に、そのシンク フォルダーの内容を切り捨てるには、 **[Clear the folder]** (フォルダーのクリア) を選択します。

## <a name="rule-based-mapping"></a>ルール ベースのマッピング
自動マッピングを無効にすると、列ベースのマッピング (固定マッピング) とルール ベースのマッピングのいずれかを追加するオプションを利用できます。 ルール ベースのマッピングでは、パターン マッチングを使用した式を作成できます。 

![ルール ベースのマッピング](media/data-flow/rules4.png "ルール ベースのマッピング")

ルール ベースのマッピングを選択すると、ADF に対して、受信パターン ルールに一致するように一致式を評価し、送信フィールド名を定義するように指示します。 フィールドとルールベースの両方のマッピングの任意の組み合わせを追加することもできます。 フィールド名は、ソースからの受信メタデータに基づいて、実行時に ADF によって生成されます。 生成されたフィールドの名前は、デバッグ中に表示することも、[データ プレビュー] ウィンドウを使用して表示することもできます。

パターン マッチングの詳細については、[列パターンに関するドキュメント](concepts-data-flow-column-pattern.md)を参照してください。

## <a name="file-name-options"></a>ファイル名のオプション

ファイルの名前付けを設定します。 

   * **既定**:Spark は PART の既定値に基づいて、ファイルに名前を付けることができます。
   * **パターン**:出力ファイルのパターンを入力します。 たとえば、**loans[n]** と入力すると、loans1.csv、loans2.csv のように作成されます。
   * **Per partition**(パーティションあたり):パーティションごとに 1 つのファイル名を入力します。
   * **As data in column**(列内のデータとして):出力ファイルを列の値に設定します。
   * **Output to a single file**(1 つのファイルに出力する):このオプションを使用すると、ADF は、パーティション分割された出力ファイルを単一の名前付きファイルに結合します。 このオプションを使用するには、データセットは、フォルダー名に解決する必要があります。 また、このマージ操作はノード サイズに基づいて失敗する可能性があることに注意してください。

> [!NOTE]
> Data Flow の実行アクティビティを実行している場合にのみ、ファイル操作は開始します。 これらは、Data Flow のデバッグ モードでは開始しません。

## <a name="database-options"></a>データベース オプション

データベース設定を選択します。

![SQL シンクのオプションを示す [設定] タブ](media/data-flow/alter-row2.png "SQL オプション")

* **Update method**(更新方法):既定では、挿入を許可します。 ソースからの新しい行の挿入を停止させる場合は、 **[Allow insert]** (挿入の許可) をオフにします。 行を更新、アップサート、または削除するには、最初に、それらのアクションに対して行をタグ付けするための行の変更変換を追加します。 
* **Recreate table**(テーブルの再作成):データ フローが完了する前に、対象のテーブルを破棄したり作成します。
* **[Truncate table]** (テーブルの切り詰め):データ フローが完了する前に、対象のテーブルからすべて行を削除します。
* **バッチ サイズ**: 書き込みをチャンクにまとめる数値を入力します。 大量のデータの読み込みにこのオプションを使用します。 
* **Enable staging**(ステージングの有効化):シンク データセットとして Azure Data Warehouse を読み込むときに、PolyBase を使用します。
* **[Pre and Post SQL scripts] (事前および事後 SQL スクリプト)** : データがシンク データベースに書き込まれる前 (前処理) と書き込まれた後 (後処理) に実行される複数行の SQL スクリプトを入力します。

![事前および事後 SQL 処理スクリプト](media/data-flow/prepost1.png "SQL 処理スクリプト")

> [!NOTE]
> Data Flow で、ターゲット データベースに新しいテーブル定義を作成するように Data Factory に指示できます。 テーブル定義を作成するには、新しいテーブル名を持つシンク変換でデータセットを設定します。 SQL データセットのテーブル名の下で、 **[編集]** をクリックして新しいテーブル名を入力します。 次に、シンク変換で、 **[Allow schema drift]** (スキーマの誤差を許可) をオンにします。 **[スキーマのインポート]** を **[なし]** に設定します。

![テーブル名を編集する場所を示した、SQL データセット設定](media/data-flow/dataset2.png "SQL スキーマ")

> [!NOTE]
> データベース シンク内の行を更新または削除するときに、キー列を設定する必要があります。 この設定は、行の変更変換で、データ移動ライブラリ (DML) の一意の行を決定できるようにします。

## <a name="next-steps"></a>次の手順
これでデータ フローが作成されたので、[Data Flow アクティビティをパイプラインに](concepts-data-flow-overview.md)追加します。
