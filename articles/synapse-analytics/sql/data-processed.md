---
title: サーバーレス SQL プールのコスト管理
description: このドキュメントでは、サーバーレス SQL プールのコストを管理する方法と、Azure Storage でデータ クエリを実行したときにデータ処理量が計算される方法について説明します。
services: synapse analytics
author: filippopovic
ms.service: synapse-analytics
ms.topic: conceptual
ms.subservice: sql
ms.date: 11/05/2020
ms.author: fipopovi
ms.reviewer: jrasnick
ms.openlocfilehash: 8a26f8ced5e91810f8cadff0a27796dc817e6517
ms.sourcegitcommit: f28ebb95ae9aaaff3f87d8388a09b41e0b3445b5
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/29/2021
ms.locfileid: "94491571"
---
# <a name="cost-management-for-serverless-sql-pool-in-azure-synapse-analytics"></a>Azure Synapse Analytics でのサーバーレス SQL プールのコスト管理

この記事では、Azure Synapse Analytics でサーバーレス SQL プールのコストを見積もって管理する方法について説明します。
- クエリを発行する前に、処理されるデータの量を見積もる
- コスト管理機能を使用して予算を設定する

Azure Synapse Analytics のサーバーレス SQL プールのコストは、Azure の月額請求料金の一部でしかないことを理解してください。 Azure の他のサービスを使用している場合は、サードパーティのサービスを含め、お使いの Azure サブスクリプションで使用されている Azure のすべてのサービスとリソースに対して課金されます。 この記事では、Azure Synapse Analytics でサーバーレス SQL プールのコストを計画して管理する方法について説明します。

## <a name="data-processed"></a>データ処理量

"*データ処理量*" は、クエリが実行される間にシステムによって一時的に保存されるデータの量です。 データ処理量は、次の量で構成されます。

- ストレージから読み取られたデータの量。 この量には以下が含まれます。
  - データの読み取り中に読み取られるデータ。
  - メタデータの読み取り中に読み取られるデータ (Parquet など、メタデータが含まれているファイル形式の場合)。
- 中間結果におけるデータの量。 このデータは、クエリの実行中にノード間で転送されます。 ここには、自分のエンドポイントへの (非圧縮形式での) データ転送が含まれています。 
- ストレージに書き込まれたデータの量。 CETAS を使用して結果セットをストレージにエクスポートする場合は、書き出されたデータの量が、CETAS の SELECT 部分で処理されたデータの量に追加されます。

ストレージからのファイルの読み取りは、高度に最適化されています。 このプロセスでは以下が使用されます。

- プリフェッチ。これにより、読み取られたデータの量にオーバーヘッドが追加される可能性があります。 クエリによってファイル全体が読み取られる場合は、オーバーヘッドは発生しません。 上位 N 個のクエリなど、ファイルが部分的に読み取られる場合は、プリフェッチの使用によって読み取られるデータが少し増えます。
- 最適化されたコンマ区切り値 (CSV) パーサー。 PARSER_VERSION='2.0' を使用して CSV ファイルを読み取る場合は、ストレージから読み取られるデータの量がわずかに増えます。 最適化された CSV パーサーでは、ファイルが均等なサイズのチャンクに分けられ、並列して読み取られます。 チャンクは必ずしも行全体を含みません。 すべての行が解析されるように、最適化された CSV パーサーでは、隣接するチャンクの小さなフラグメントも読み取られます。 このプロセスでは、少量のオーバーヘッドが発生します。

## <a name="statistics"></a>統計

サーバーレス SQL プールのクエリ オプティマイザーでは、統計に基づいて最適なクエリ実行プランを生成します。 統計は手動で作成できます。 そうでない場合は、サーバーレス SQL プールによって自動的に作成されます。 どちらの場合も、指定のサンプル レートで特定の列を返す独立したクエリを実行して統計が作成されます。 このクエリには、処理されたデータの量が関連付けられています。

作成した統計からメリットが得られる同じまた別のクエリを実行すると、可能な場合に統計が再利用されます。 統計の作成で追加のデータ処理量が発生することはありません。

統計が Parquet 列用に作成された場合は、関連する列のみがファイルから読み取られます。 統計が CSV 列用に作成された場合は、ファイル全体が読み取られて解析されます。

## <a name="rounding"></a>丸め

処理されたデータの量は、クエリごとに最も近い MB に丸められます。 各クエリでは、少なくとも 10 MB のデータが処理されます。

## <a name="what-data-processed-doesnt-include"></a>データ処理量に含まれない内容

- サーバーレベルのメタデータ (ログイン、ロール、サーバーレベルの資格情報など)。
- 自分のエンドポイントに作成するデータベース。 これらのデータベースに含まれるのはメタデータのみです (例: ユーザー、ロール、スキーマ、ビュー、インライン テーブル値関数 (TVF)、ストアド プロシージャ、データベース スコープ資格情報、外部データ ソース、外部ファイル形式、外部テーブル)。
  - スキーマ推論を使用すると、ファイルのフラグメントが読み取られて、列名とデータ型が推論されます。読み取られたデータの量は、処理されたデータの量に追加されます。
- データ定義言語 (DDL) ステートメント。ただし、指定されたサンプルの割合に基づいてストレージからのデータが処理される CREATE STATISTICS ステートメントは除きます。
- メタデータのみのクエリ。

## <a name="reducing-the-amount-of-data-processed"></a>処理されるデータの量の削減

データをパーティション分割し、圧縮列ベースの形式 (Parquet など) に変換することで、処理されるデータの量をクエリごとに最適化してパフォーマンスを改善できます。

## <a name="examples"></a>例

3 つのテーブルがあると想定します。

- population_csv テーブルは 5 TB 分の CSV ファイルに基づいています。 ファイルは、サイズの等しい 5 つの列で構成されます。
- population_parquet テーブルには、population_csv テーブルと同じデータが含まれています。 これは 1 TB 分の Parquet ファイルに基づいています。 このテーブルは前のものよりも小さくなっています。これは、データが Parquet 形式で圧縮されているためです。
- very_small_csv テーブルは、100 KB 分の CSV ファイルに基づいています。

**クエリ 1**:SELECT SUM(population) FROM population_csv

このクエリでは、ファイル全体を読み取って解析し、母集団列の値を取得します。 ノードによってこのテーブルのフラグメントが処理され、各フラグメントの母集団の合計がノード間で転送されます。 最終的な合計は自分のエンドポイントに転送されます。 

このクエリでは、5 TB のデータが処理されるほか、フラグメントの合計を転送するために少量のオーバーヘッドが発生します。

**クエリ 2**:SELECT SUM(population) FROM population_parquet

Parquet など、圧縮列ベースの形式でクエリを実行すると、クエリ 1 よりも読み取られるデータが少なくなります。 サーバーレス SQL プールによってファイル全体ではなく 1 つの圧縮列が読み取られるため、この結果が表示されます。 この場合、0.2 TB が読み取られます。 (サイズの等しい 5 つの列はそれぞれ 0.2 TB です。)ノードによってこのテーブルのフラグメントが処理され、各フラグメントの母集団の合計がノード間で転送されます。 最終的な合計は自分のエンドポイントに転送されます。 

このクエリでは、0.2 TB のデータが処理されるほか、フラグメントの合計を転送するために少量のオーバーヘッドが発生します。

**クエリ 3**:SELECT * FROM population_parquet

このクエリでは、すべての列を読み取り、すべてのデータを非圧縮形式で転送します。 圧縮形式が 5:1 の場合、このクエリでは 1 TB が読み取られて 5 TB の非圧縮データが転送されるため、6 TB が処理されます。

**クエリ 4**:SELECT COUNT(*) FROM very_small_csv

このクエリでは、ファイル全体を読み取ります。 このテーブルのために保存されているファイルの合計サイズは 100 KB です。 ノードによってこのテーブルのフラグメントが処理され、各フラグメントの合計がノード間で転送されます。 最終的な合計は自分のエンドポイントに転送されます。 

このクエリでは、100 KB をわずかに上回るデータが処理されます。 このクエリで処理されるデータの量は、この記事の「[丸め](#rounding)」セクションで指定されているとおり、10 MB に丸められます。

## <a name="cost-control"></a>コスト管理

サーバーレス SQL プールのコスト管理機能を使用すると、処理されるデータの量について予算を設定できます。 1 日、1 週間、1 か月の期間で、データ処理量の予算を TB 単位で設定できます。 同時に、1 つまたは複数の予算を設定できます。 サーバーレス SQL プールのコスト管理を構成するには、Synapse Studio または T-SQL を使用します。

## <a name="configure-cost-control-for-serverless-sql-pool-in-synapse-studio"></a>Synapse Studio におけるサーバーレス SQL プールのコスト管理の構成
 
Synapse Studio でサーバーレス SQL プールのコスト管理を構成するには、左側にあるメニューの [管理] 項目に移動し、[Analytics プール] の [SQL プール] 項目を選択します。 サーバーレス SQL プールにカーソルを合わせると、コスト管理のアイコンが表示されるので、そのアイコンをクリックします。

![コスト管理のナビゲーション](./media/data-processed/cost-control-menu.png)

コスト管理アイコンをクリックすると、サイド バーが表示されます。

![コスト管理の構成](./media/data-processed/cost-control-sidebar.png)

1 つまたは複数の予算を設定するには、設定したい予算の [有効化] を最初にクリックしてから、テキスト ボックスに整数値を入力します。 値の単位は TB です。 目的の予算を構成したら、サイド バーの下部にある [適用] ボタンをクリックします。 これで、予算が設定されました。

## <a name="configure-cost-control-for-serverless-sql-pool-in-t-sql"></a>T-SQL におけるサーバーレス SQL プールのコスト管理の構成

T-SQL でサーバーレス SQL プールのコスト管理を構成するには、次のストアド プロシージャを 1 つまたは複数実行する必要があります。

```sql
sp_set_data_processed_limit
    @type = N'daily',
    @limit_tb = 1

sp_set_data_processed_limit
    @type= N'weekly',
    @limit_tb = 2

sp_set_data_processed_limit
    @type= N'monthly',
    @limit_tb = 3334
```

現在の構成を確認するには、次の T-SQL ステートメントを実行します。

```sql
SELECT * FROM sys.configurations
WHERE name like 'Data processed %';
```

現在の日、週、または月の期間で処理されたデータの量を確認するには、次の T-SQL ステートメントを実行します。

```sql
SELECT * FROM sys.dm_external_data_processed
```

## <a name="next-steps"></a>次のステップ

クエリのパフォーマンスを最適化する方法については、[サーバーレス SQL プールのベスト プラクティス](best-practices-sql-on-demand.md)に関するページを参照してください。
