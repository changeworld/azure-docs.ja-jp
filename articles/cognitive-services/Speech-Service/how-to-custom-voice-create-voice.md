---
title: Custom Voice を作成する - Speech サービス
titleSuffix: Azure Cognitive Services
description: 実際のデータをアップロードする準備ができたら、Custom Voice ポータルに移動します。 Custom Voice プロジェクトを作成するか、選択します。 このプロジェクトでは、実際の音声トレーニングに使用するデータとして適切な言語/ロケールと性別プロパティを共有する必要があります。
services: cognitive-services
author: erhopf
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 11/04/2019
ms.author: erhopf
ms.openlocfilehash: bbe1d651a7d2d2cac1b1aa78b815b2797ad185c5
ms.sourcegitcommit: 2ec4b3d0bad7dc0071400c2a2264399e4fe34897
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/28/2020
ms.locfileid: "76717318"
---
# <a name="create-a-custom-voice"></a>Custom Voice を作成する

[Custom Voice 用のデータの準備](how-to-custom-voice-prepare-data.md)に関するページでは、カスタム音声のトレーニングに使用できるさまざまなデータの種類と、さまざまな形式の要件について説明しました。 実際のデータが準備できたら、[Custom Voice ポータル](https://aka.ms/custom-voice-portal)に、または Custom Vision Training API を使用して、そのデータのアップロードを開始できます。 ここでは、ポータルを使用したカスタム音声のトレーニング手順を説明します。

> [!NOTE]
> このページでは、「[Custom Voice の概要](how-to-custom-voice.md)」と[カスタム音声用のデータの準備](how-to-custom-voice-prepare-data.md)に関するページを読み、Custom Voice プロジェクトを作成していることを前提としています。

[カスタマイズ用の言語](language-support.md#customization)に関するセクションで、カスタム音声用にサポートされている言語を確認してください。

## <a name="upload-your-datasets"></a>データセットをアップロードする

実際のデータをアップロードする準備ができたら、[Custom Voice ポータル](https://aka.ms/custom-voice-portal)に移動します。 Custom Voice プロジェクトを作成するか、選択します。 このプロジェクトでは、実際の音声トレーニングに使用するデータとして適切な言語/ロケールと性別プロパティを共有する必要があります。 たとえば、英国アクセントの英語で音声を録音した場合は `en-GB` を選択します。

**[データ]** タブに移動し、 **[データのアップロード]** をクリックします。 ウィザードで、準備したものと一致する正しいデータの種類を選択します。

アップロードする各データセットでは、選択したデータの種類の要件が満たされている必要があります。 アップロードする前に、データを正しく書式設定することが重要です。 これにより、データが Custom Voice サービスによって確実に処理されます。 [Custom Voice 用のデータの準備](how-to-custom-voice-prepare-data.md)に関するページに移動し、実際のデータが正しく書式設定されていることを確認します。

> [!NOTE]
> Free サブスクリプション (F0) ユーザーは、2 個のデータセットを同時にアップロードできます。 Standard サブスクリプション (S0) ユーザーは、5 個のデータセットを同時にアップロードできます。 制限に達した場合は、少なくとも 1 つのデータセットのインポートが終わるまで待機します。 その後、やり直してください。

> [!NOTE]
> サブスクリプションあたりのインポートできるデータセットの最大数は、Free サブスクリプション (F0) ユーザーの場合は .zip ファイル 10 個、Standard サブスクリプション (S0) ユーザーの場合は 500 個です。

アップロード ボタンを押すと、データセットが自動的に検証されます。 データ検証には、ファイル形式、サイズ、サンプリング レートを確認する、オーディオ ファイルの一連のチェックが含まれます。 エラーが見つかった場合は、修正して、もう一度送信します。 データのインポート要求が正常に開始されると、先ほどアップロードしたデータセットに対応するエントリがデータの表に表示されます。

次の表に、インポートされたデータセットの処理状態を示します。

| State | 意味 |
| ----- | ------- |
| 処理中 | ご自分のデータセットは受信され、処理されています。 |
| 成功 | ご自分のデータセットは検証が済み、音声モデルの作成に使用できるようになっています。 |
| 失敗 | ファイルのエラー、データの問題、ネットワークの問題など、さまざまな理由により、処理中にご自分のデータセットが失敗しました。 |

検証が完了すると、ご自分の各データセットについて、一致した発話の合計数を **[Utterances]\(発話\)** 列で確認できます。 選択したデータの種類では長いオーディオのセグメント化が必要な場合、この列には、実際のトランスクリプトに基づいて、または音声文字起こしサービスを通じて、自動的にセグメント化された発話のみが反映されます。 さらに、検証済みのデータセットをダウンロードして、正常にインポートされた発話とそのマッピング トランスクリプトの詳細な結果を確認できます。 ヒント: 長いオーディオのセグメント化では、データ処理が完了するまでに 1 時間以上かかることがあります。

en-US と zh-CN のデータセットでは、さらに、レポートをダウンロードして、実際の各録音の発音スコアとノイズ レベルを確認できます。 発音スコアの範囲は 0 ～ 100 です。 スコアが 70 未満の場合は、通常、音声のエラーまたはスクリプトの不一致を示しています。 アクセントが強いと発音スコアが下がることがあり、生成されるデジタル音声に影響します。

高い信号雑音比 (SNR) は、オーディオのノイズが低いことを示します。 一般に、専門スタジオでの録音によって、SNR が 50 以上に達するようにできます。 SNR が 20 未満のオーディオでは、生成される音声に明らかなノイズが含まれる可能性があります。

発音スコアが低い場合や SNR が悪い場合は、発話を録音し直すことを検討してください。 再録音できない場合は、それらの発話をデータセットから除外してもかまいません。

## <a name="build-your-custom-voice-model"></a>ご自分のカスタム音声モデルを作成する

ご自分のデータセットの検証後、それを使用してご自分のカスタム音声モデルを作成できます。

1.  **[Text-to-Speech]\(テキスト読み上げ\)、[Custom Voice]、[トレーニング]** の順に移動します。

2.  **[Train model]\(モデルのトレーニング\)** をクリックします。

3.  次に、このモデルを識別しやすい**名前**と**説明**を入力します。

    名前は慎重に選択します。 ここで入力する名前が、SSML 入力の一部としての音声合成の要求時に、音声を指定するために使用する名前になります。 アルファベット、数字、およびいくつかの区切り文字 (-、\_、(、) など) だけを使用できます。 音声モデルごとに、異なる名前を使用します。

    **[Description]\(説明)** フィールドの一般的な用途は、モデルの作成に使用されたデータセットの名前を記録することです。

4.  **[Select training data]\(トレーニング データの選択\)** ページから、トレーニングに使用する 1 つまたは複数のデータセットを選択します。 送信前に、発話の数を確認します。 en-US と zh-CN の音声モデルについては、任意の数の発話から始めることができます。 その他のロケールについては、音声をトレーニングするには、2,000 を超える発話を選択する必要があります。

    > [!NOTE]
    > 重複したオーディオ名はトレーニングから削除されます。 選択したデータセット内の複数の .zip ファイルに同じオーディオ名が含まれていないことを確認してください。

    > [!TIP]
    > 高品質の結果を得るためには、同じ話者のデータセットを使用する必要があります。 トレーニング用に送信したデータセットに含まれている明確な発話の合計数が 6,000 未満の場合、統計的パラメトリック音声合成手法を使用して、ご自分の音声モデルをトレーニングします。 実際のトレーニング データの明確な発話の合計数が 6,000 を超えている場合は、連結音声合成手法を使用してトレーニング プロセスを開始します。 通常、連結手法では、自然で忠実性の高い音声結果を得ることができます。 一般公開されている[ニューラル音声](language-support.md#neural-voices)と同等のデジタル音声を生成できる最新のニューラル TTS テクノロジを使用したモデルのトレーニングをご希望の場合は、[Custom Voice チームにお問い合わせください](https://go.microsoft.com/fwlink/?linkid=2108737)。

5.  **[Train]\(トレーニング\)** をクリックして、実際の音声モデルの作成を開始します。

[トレーニング] の表に、この新しく作成されたモデルに対応する新しいエントリが表示されます。 この表には、次の状態も表示されます。処理中、成功、失敗。

表示される状態は、ここに示すように、ご自分のデータセットから音声モデルへの変換プロセスを反映しています。

| State | 意味 |
| ----- | ------- |
| 処理中 | 実際の音声モデルを作成中です。 |
| 成功 | 実際の音声モデルは作成が済み、デプロイ可能です。 |
| 失敗 | 気が付かなかったデータの問題やネットワークの問題など、さまざまな理由により、トレーニング中に実際の音声モデルが失敗しました。 |

トレーニング時間は、処理されるオーディオ データの量によって異なります。 標準的な時間は、数百個の発話で約 30 分、20,000 個の発話で 40 時間です。 実際のモデルのトレーニングが成功したら、そのテストを開始できます。

> [!NOTE]
> Free サブスクリプション (F0) ユーザーは、1 つの音声フォントを同時にトレーニングできます。 Standard サブスクリプション (S0) ユーザーは、3 つの音声を同時にトレーニングできます。 制限に達した場合は、少なくとも 1 つの音声フォントのトレーニングが終わるまで待ってから、やり直します。

> [!NOTE]
> サブスクリプションあたりのトレーニングできる音声モデルの最大数は、Free サブスクリプション (F0) ユーザーの場合はモデル 10 個、Standard サブスクリプション (S0) ユーザーの場合は 100 個です。

ニューラル音声トレーニング機能を使用している場合、リアルタイムのストリーミング シナリオ向けに最適化されたモデルをトレーニングするか、または非同期の[長いオーディオ合成](long-audio-api.md)用に最適化された HD ニューラル モデルをトレーニングするかを選択できます。  

## <a name="test-your-voice-model"></a>実際の音声モデルをテストする

音声フォントが正常に作成されたら、使用のために展開する前にテストすることができます。

1.  **[Text-to-Speech]\(テキスト読み上げ\)、[Custom Voice]、[Testing]\(テスト\)** の順に移動します。

2.  **[テストの追加]** をクリックします。

3.  テストする 1 つまたは複数のモデルを選択します。

4.  その音声で読み上げるテキストを指定します。 一度に複数のモデルをテストすることを選択した場合、異なるモデルのテストに同じテキストが使用されます。

    > [!NOTE]
    > テキストの言語は、音声フォントの言語と同じである必要があります。 テストできるのは、正常にトレーニングされたモデルのみです。 この手順では、プレーンテキストのみがサポートされます。

5.  **Create** をクリックしてください。

実際のテスト要求を送信すると、テスト ページに戻ります。 これでテーブルには、新しい要求に対応するエントリと状態の列が表示されます。 音声の合成には数分かかる場合があります。 状態の列に **[成功]** と表示されたら、オーディオを再生するか、テキスト入力 (.txt ファイル) とオーディオ出力 (.wav ファイル) をダウンロードし、さらに、後者を試聴して品質を確認できます。

また、テスト用に選択した各モデルの詳細ページで、テスト結果を確認することもできます。 **[トレーニング]** タブに移動し、モデル名をクリックして、モデルの詳細ページに入ります。

## <a name="create-and-use-a-custom-voice-endpoint"></a>カスタム音声エンドポイントを作成して使用する

音声モデルの作成とテストが正常に終了したら、カスタム Text-to-Speech エンドポイントに展開します。 その後は、REST API で Text-to-Speech 要求を行うときの通常のエンドポイントの代わりに、このエンドポイントを使います。 ご自分のカスタム エンドポイントは、フォントをデプロイするときに使ったサブスクリプションからのみ呼び出すことができます。

新しいカスタム音声エンドポイントを作成するには、 **[Text-to-Speech]\(テキスト読み上げ\)、[Custom Voice]、[Deployment]\(デプロイ\)** の順に移動します。 **[エンドポイントの追加]** を選択し、ご自分のカスタム エンドポイントの**名前**と**説明**を入力します。 次に、このエンドポイントに関連付けるカスタム音声モデルを選択します。

**[追加]** をクリックすると、エンドポイントの表にご自分の新しいエンドポイントのエントリが表示されます。 新しいエンドポイントのインスタンス化には、数分かかることがあります。 展開の状態が **[Succeeded]\(成功\)** の場合、エンドポイントを使用する準備ができています。

> [!NOTE]
> Free サブスクリプション (F0) ユーザーは、1 つだけモデルをデプロイできます。 Standard サブスクリプション (S0) ユーザーは、それぞれが独自のカスタム音声を使用する最大 50 個のエンドポイントを作成できます。

> [!NOTE]
> 実際のカスタム音声を使用するには、音声モデルの名前を指定し、HTTP 要求に直接カスタム URI を使用し、同じサブスクリプションを使用して TTS サービスの認証を通過する必要があります。

ご自分のエンドポイントがデプロイされると、エンドポイント名はリンクとして表示されます。 リンクをクリックすると、エンドポイント キー、エンドポイントの URL、サンプル コードなど、ご自分のエンドポイントに固有の情報が表示されます。

Custom Voice ポータルを使用して、エンドポイントのオンライン テストを行うこともできます。 ご自分のエンドポイントをテストするには、 **[Endpoint detail]\(エンドポイントの詳細\)** ページから **[Check endpoint]\(エンドポイントの確認\)** を選択します。 エンドポイントのテスト ページが表示されます。 読み上げるテキストをテキスト ボックスに (プレーンテキストまたは [SSML 形式](speech-synthesis-markup.md)のどちらかで) 入力します。 カスタム音声フォントで読み上げられるテキストを聞くには、 **[Play]\(再生\)** を選択します。 このテスト機能は、カスタム音声合成の実際の使用量に対して課金されます。

カスタム エンドポイントの機能は、テキスト読み上げ要求に使用される標準のエンドポイントと同じです。 詳しくは、[REST API](rest-text-to-speech.md) に関するページをご覧ください。

## <a name="next-steps"></a>次のステップ

* [ガイド:音声サンプルを録音する](record-custom-voice-samples.md)
* [Text-to-Speech API リファレンス](rest-text-to-speech.md)
* [Long Audio API](long-audio-api.md)
