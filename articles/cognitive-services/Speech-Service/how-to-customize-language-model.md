---
title: 'チュートリアル: Speech Service を使用して言語モデルを作成する方法'
titlesuffix: Azure Cognitive Services
description: Speech Service を使用して言語モデルを作成する方法について説明します。 このカスタム言語モデルを Microsoft が提供する既存の最先端の音声モデルと組み合わせて使用して、アプリケーションに音声操作を追加します。
services: cognitive-services
author: PanosPeriorellis
manager: cgronlun
ms.service: cognitive-services
ms.component: speech-service
ms.topic: tutorial
ms.date: 12/06/2018
ms.author: panosper
ms.custom: seodec18
ms.openlocfilehash: 0eb946babaa3a01ca933a1290122755978fa017b
ms.sourcegitcommit: 9fb6f44dbdaf9002ac4f411781bf1bd25c191e26
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 12/08/2018
ms.locfileid: "53093467"
---
# <a name="tutorial-create-a-custom-language-model"></a>チュートリアル: カスタム言語モデルを作成する

このドキュメントでは、カスタム言語モデルを作成します。 このカスタム言語モデルを Microsoft が提供する既存の最先端の音声モデルと組み合わせて使用して、アプリケーションに音声操作を追加できます。

このドキュメントでは、以下の方法を説明します。
> [!div class="checklist"]
> * データを準備する
> * 言語データ セットをインポートする
> * カスタム言語モデルを作成する

Cognitive Services アカウントをお持ちでない場合は、開始する前に [無料アカウント](https://azure.microsoft.com/try/cognitive-services/) を作成してください。

## <a name="prerequisites"></a>前提条件

Cognitive Services アカウントがサブスクリプションに接続されていることを確認するため、[[Cognitive Services サブスクリプション]](https://customspeech.ai/Subscriptions) ページを開きます。

Azure portal で作成された Speech Services サブスクリプションに接続するため、**[Connect existing subscription]\(既存のサブスクリプションに接続する\)** ボタンを選択します。

Azure portal での Speech Services サブスクリプションの作成については、[概要](get-started.md)ページを参照してください。

## <a name="prepare-the-data"></a>データを準備する

アプリケーションのためにカスタム言語モデルを作成するには、以下のような発話例の一覧をシステムに提供する必要があります。

*   "患者は先週じんましんが出ました。"
*   "患者には、完治したヘルニア手術の傷跡があります。"

文は、完全である必要も、文法的に正しい必要もありませんが、展開の中でシステムが遭遇すると予想される発話の入力を正確に反映している必要があります。 これらの例は、ユーザーがアプリケーションを使用して実行するタスクのスタイルと内容の両方を反映している必要があります。

言語モデルのデータは、UTF-8 BOM で記述する必要があります。 テキスト ファイルには、1 行につき 1 つの用例 (文、発言、またはクエリ) を含める必要があります。

特定の用語の重み (重要度) を高くしたい場合は、データに、その用語を含む発話をいくつか追加することができます。

言語データの主な要件を次の表にまとめます。

| プロパティ | 値 |
|----------|-------|
| テキストのエンコード | UTF-8 BOM|
| 1 行あたりの発話の数 | 1 |
| ファイルの最大サイズ | 1.5 GB |
| 解説 | 文字を 5 回以上繰り返さないようにします (例: 'aaaaa')|
| 解説 | '\t' などの特殊文字や、[Unicode 文字コード表](http://www.utf8-chartable.de/) の U+00A1 より上のその他の UTF-8 文字は使用しません|
| 解説 | 一意に発音する方法がないため、URI も拒否されます|

テキストのインポート時に、システムで処理できるように、テキストの正規化が行われます。 ただし、"_データをアップロードする前_" にユーザーが行う必要があるいくつかの重要な正規化があります。 [文字起こしガイドライン](prepare-transcription.md)を参照して、言語データを準備する際に使用するのが適切な言語を判断してください。

## <a name="language-support"></a>言語のサポート

**音声テキスト変換**のカスタム言語モデルで[サポートされる言語](language-support.md#text-to-speech)の完全な一覧を参照してください。



## <a name="import-the-language-data-set"></a>言語データ セットをインポートする

**[Language Datasets]\(言語データセット\)** 行の **[インポート]** ボタンを選択すると、サイトには、新しいデータ セットをアップロードするためのページが表示されます。

言語データ セットをインポートする準備ができたら、[Speech Services ポータル](https://customspeech.ai) にサインインします。 まず、上部のリボンの **[Custom Speech]** ドロップダウン メニューを選択します。 次に、**適応データ** を選択します。 初めて Speech Services にデータをアップロードしようとすると、 **Datasets** という名前の空のテーブルが表示されます。

新しいデータ セットをインポートするには、**言語データセット** 行の **インポート** ボタンを選択します。 次に、サイトには新しいデータ セットをアップロードするためのページが表示されます。 今後そのデータ セットを識別しやすいように**名前**と**説明**を入力してから、ロケールを選択します。

次に、**[ファイルの選択]** ボタンを使用して、言語データのテキスト ファイルを見つけます。 その後、**[インポート]** をクリックすると、データ セットがアップロードされます。 データ セットのサイズによっては、インポートに数分かかる場合があります。

![試す](media/stt/speech-language-datasets-import.png)

インポートが完了すると、言語データに、その言語データ セットに対応するエントリが含まれています。 一意の ID (GUID) が割り当てられることに注意してください。 データには、現在の状態を反映するステータスもあります。 そのステータスは、処理を待っている間は **[待機中]** で、検証を受けている間は **[処理中]**、データを使用する準備ができると **[完了]** です。 データの検証では、ファイル内のテキストに対する一連のチェックが実行されます。 データのテキストの、いくらかの正規化も行います。

ステータスが **[完了]** のときには、**[レポートの表示]** を選択し、言語データ検証レポートを表示できます。 失敗した発言の詳細と共に、検証に成功した発言の数と失敗した発言の数が表示されます。 次の例では、正しくない文字のために 2 つの例が検証に失敗しました。 (このデータ セット内では、最初の行に 2 つのタブ文字があり、2 行目には印刷可能な ASCII 文字セットの一部ではない文字がいくつかあり、3 行目は空白でした。)

![試す](media/stt/speech-language-datasets-report.png)

言語データ セットのステータスが **[完了]** のときには、それを使用してカスタム言語モデルを作成できます。

![試す](media/stt/speech-language-datasets.png)

## <a name="create-a-custom-language-model"></a>カスタム言語モデルを作成する

言語データが準備できたら、**[メニュー]** ドロップダウン メニューから **[言語モデル]** を選択して、カスタム言語モデルの作成プロセスを開始します。 このページには、現在のカスタム言語モデルを持つ **[言語モデル]** という名前のテーブルが含まれています。 カスタム言語モデルをまだ作成していない場合、テーブルは空になります。 現在のロケールは、テーブル内で関連するデータ エントリの横に表示されます。

アクションを実行する前に、適切なロケールを選択する必要があります。 現在のロケールは、データ、モデル、デプロイのページすべてでテーブル タイトルに示されます。 ロケールを変更するには、テーブル タイトルの下にある **[Change Locale]\(ロケールの変更\)** ボタンを選択します。  これでロケールの確認ページに移動します。 テーブルに戻るには **[OK]** を選択します。

[言語モデルの作成] ページで、このモデルの関連情報 (使用されているデータ セットなど) を追跡するために役立つ **[名前]** と **[説明]** を入力します。 次に、ドロップダウン メニューから **[Base Language Model]\(ベース言語モデル\)** を選択します。 このモデルがカスタマイズの開始点です。

2 つのベース言語モデルから選択できます。 Search and Dictation モデルは、コマンド、検索クエリ、ディクテーションなどのアプリケーションに向けられた音声に適しています。 Conversational モデルは、会話形式で話される音声の認識に適しています。 この種類の音声は、通常は他の人に向けて発せられるものであり、コール センターや会議で発生します。

Search and Dictation モデルは、コマンド、検索クエリ、ディクテーションなどのアプリケーションに向けられた音声に適しています。 Conversational モデルは、会話形式で話される音声の認識に適しています。 この種類の音声は、通常は他の人に向けて発せられるものであり、コール センターや会議で発生します。 "Universal" という新しいモデルも一般公開されています。 Universal は、すべてのシナリオに取り組んで、最終的に Search and Dictation モデルや Conversational モデルを置き換えることを目的としています。

次の例に示すように、ベース言語モデルを指定した後、**[Language Data]\(言語データ\)** ドロップダウン メニューを使用してカスタマイズに使用する言語データ セットを選択します。

![試す](media/stt/speech-language-models-create2.png)

音響モデルの作成と同じように、処理が完了したら、必要に応じて新しいモデルのオフライン テストを行うことを選択できます。 モデルの評価には、音響データ セットが必要です。

言語モデルのオフライン テストを行うには、**[オフライン テスト]** の横にあるチェック ボックスを選択します。 次に、ドロップダウン メニューから音響モデルを選択します。 カスタム音響モデルを作成していない場合は、Microsoft ベースの音響モデルがメニューの唯一のモデルになります。 Conversational LM ベース モデルを選択した場合は、ここで Conversational AM を使用する必要があります。 Search and Dictation LM モデルを使用する場合は、Search and Dictate AM を選択する必要があります。

最後に、評価を行うために使用する音響データ セットを選択します。

処理を開始する準備ができたら、**[作成]** を選択します。 次に、言語モデルのテーブルが表示されます。 このモデルに対応するテーブルに新しいエントリがあります。 このステータスは、モデルの状態を反映しており、**[待機中]**、**[処理中]**、**[完了]** など、いくつかの状態を移行してゆきます。

モデルは、**[完了]** 状態に達したら、エンドポイントに展開することができます。 **[結果の表示]** を選択すると、オフライン テストの結果が表示されます (実行した場合)。

ある時点でモデルの **[名前]** または **[説明]** を変更する場合は、言語モデル テーブルの適切な行の **[編集]** リンクを使用できます。

## <a name="next-steps"></a>次の手順

- [Speech Services 試用版サブスクリプションを取得する](https://azure.microsoft.com/try/cognitive-services/)
- [C# で音声を認識する方法](quickstart-csharp-dotnet-windows.md)
- [Git サンプル データ](https://github.com/Microsoft/Cognitive-Custom-Speech-Service)
