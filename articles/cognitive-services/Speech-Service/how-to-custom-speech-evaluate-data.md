---
title: Custom Speech の精度の評価と向上 - 音声サービス
titleSuffix: Azure Cognitive Services
description: このドキュメントでは、音声テキスト変換モデルまたはカスタム モデルの品質を定量的に測定し、向上させる方法について説明します。 正確性をテストするには、オーディオ + ヒューマン ラベル付け文字起こしデータが必要であり、30 分から 5 時間の典型的な音声を用意する必要があります。
services: cognitive-services
author: trevorbye
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 02/12/2021
ms.author: trbye
ms.openlocfilehash: b7e4ea586098ea3eb0dfd684650f798d7988e18b
ms.sourcegitcommit: f28ebb95ae9aaaff3f87d8388a09b41e0b3445b5
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/30/2021
ms.locfileid: "100634585"
---
# <a name="evaluate-and-improve-custom-speech-accuracy"></a>Custom Speech の精度の評価と向上

この記事では、Microsoft の音声テキスト変換モデルまたは独自のカスタム モデルの精度を定量的に測定し、向上させる方法について説明します。 正確性をテストするには、オーディオ + ヒューマン ラベル付け文字起こしデータが必要であり、30 分から 5 時間の典型的な音声を用意する必要があります。

## <a name="evaluate-custom-speech-accuracy"></a>カスタム音声の正確性を評価する

モデルの正確性を測定するための業界標準は、"[ワード エラー率](https://en.wikipedia.org/wiki/Word_error_rate)" (WER) です。 WER では、認識中に識別された誤った単語の数を数え、ヒューマン ラベル付けトランスクリプトで提供された単語の総数で割ります (下では N と示されています)。 最後に、その数に 100% を掛けて WER を計算します。

![WER の数式](./media/custom-speech/custom-speech-wer-formula.png)

誤りと識別された単語は、次の 3 つのカテゴリに分類されます。

* 挿入 (I): 仮説トランスクリプトに誤って追加された単語
* 削除 (D): 仮説トランスクリプトで検出されなかった単語
* 置換 (S): 参照と仮説の間で置き換えられた単語

次に例を示します。

![誤って識別された単語の例](./media/custom-speech/custom-speech-dis-words.png)

WER の測定値をローカルでレプリケートする場合は、[SCTK](https://github.com/usnistgov/SCTK) から sclite を使用できます。

## <a name="resolve-errors-and-improve-wer"></a>エラーの解決と WER の向上

アプリ、ツール、または製品で使用しているモデルの品質を評価するために、機械認識結果の WER を使用できます。 WER が 5% から 10% であれば、良質であると見なして、使用できます。 WER が 20% であれば許容範囲内ですが、追加のトレーニングが必要な場合もあるでしょう。 WER が 30% 以上であれば、低品質であることを示しており、カスタマイズとトレーニングが必要です。

エラーがどのような分布になっているかが重要です。 多くの削除エラーが発生する場合は、通常、音声信号の強度が弱いことが原因です。 この問題を解決するには、ソースに近い場所で音声データを収集する必要があります。 挿入エラーは、音声が雑音の多い環境で録音され、クロストークが存在するために認識の問題が発生している可能性があることを意味します。 置換エラーは、ドメイン固有の用語のサンプルが、ヒューマン ラベル付け文字起こしまたは関連テキストとして十分に提供されていない場合にしばしば発生します。

個々のファイルを分析することで、どの種類のエラーが存在し、どのエラーが特定のファイルに固有であるかを判断できます。 ファイル レベルで問題を理解すると、改善の対象の特定に役立ちます。

## <a name="create-a-test"></a>テストを作成する

Microsoft の音声テキスト変換ベースライン モデルまたはトレーニングしたカスタム モデルの品質をテストする場合は、2 つのモデルを並べて比較し、正確性を評価できます。 比較には、WER と認識結果が含まれます。 通常、カスタム モデルは、Microsoft のベースライン モデルと比較されます。

モデルを並べて評価するには、次のように操作します。

1. [Custom Speech ポータル](https://speech.microsoft.com/customspeech)にサインインします。
2. **[音声テキスト変換]、[Custom Speech]、[<プロジェクト名>]、[テスト]** の順に移動します。
3. **[テストの追加]** をクリックします。
4. **[Evaluate accuracy]\(正確性の評価\)** を選択します。 テストの名前と説明を設定し、オーディオ + ヒューマン ラベル付け文字起こしデータセットを選択します。
5. テストするモデルを最大で 2 つ選択します。
6. **Create** をクリックしてください。

テストが正常に作成されたら、結果を並べて比較できます。

### <a name="side-by-side-comparison"></a>並べて比較

テストが完了すると、ステータスが "*成功*" に変わり、テストに含まれている両方のモデルの WER 数値が表示されます。 テストの名前をクリックして、テスト結果のページを表示します。 この詳細ページには、データセット内のすべての発話が一覧表示され、送信されたデータセットからの文字起こしと共に、2 つのモデルの認識結果が示されます。 並べた比較を検査しやすくするために、挿入、削除、置換を含むさまざまなエラーの種類を切り替えることができます。 音声を聞きながら各列の認識結果を比較すると、ヒューマン ラベル付け文字起こしと 2 つの音声テキスト変換モデルの結果が表示されるため、どちらのモデルがニーズに合い、どのような追加のトレーニングと改善が必要かを判断できます。

## <a name="improve-custom-speech-accuracy"></a>カスタム音声の正確性を向上させる

音声認識のシナリオは、オーディオの品質と言語 (ボキャブラリと話し方) によって異なります。 次の表は、4 つの一般的なシナリオを示しています。

| シナリオ | 音質 | ボキャブラリ | 話し方 |
|----------|---------------|------------|----------------|
| コール センター | 低 (8 kHz)。1 つのオーディオ チャネルに 2 人の人間が参加可能。圧縮可能 | 狭い。ドメインと製品に固有 | 会話形式。ゆるい構成 |
| 音声アシスタント (Cortana、ドライブスルー ウィンドウなど) | 高 (16 kHz) | エンティティ ヘビー (曲のタイトル、製品、場所) | 明確に述べられる単語と語句 |
| 音声入力 (インスタント メッセージ、メモ、検索) | 高 (16 kHz) | 多様 | メモ取り |
| 動画の字幕 | 多様 (さまざまなマイクの使用、音楽の追加など) | 多様 (会議、朗読、音楽の歌詞など) | 読み上げ、事前に準備、またはゆるい構成 |

さまざまなシナリオでさまざまな品質の結果が生成されます。 次の表では、これら 4 つのシナリオのコンテンツが[ワード エラー率 (WER)](how-to-custom-speech-evaluate-data.md) でどのように評価されるかを調べます。 この表は、各シナリオで最も一般的なエラーの種類を示しています。

| シナリオ | 音声認識の品質 | 挿入エラー | 削除エラー | 置換エラー |
|----------|----------------------------|------------------|-----------------|---------------------|
| コール センター | 中 (WER 30% 未満) | 低 (バックグラウンドで他の人間が話している場合を除く) | 高くなる可能性あり。 コール センターは雑音を伴う場合があり、話者がオーバーラップしてモデルが混同される可能性がある | 中。 これらのエラーは、製品や人々の名前によって生じる可能性がある |
| 音声アシスタント | 高 (WER 10% 未満の可能性あり) | 低 | 低 | 中 (曲のタイトル、製品名、または場所による) |
| ディクテーション | 高 (WER 10% 未満の可能性あり) | 低 | 低 | 高 |
| 動画の字幕 | 動画の種類によって異なる (WER 50% 未満の可能性あり) | 低 | 音楽、ノイズ、マイクの品質によって高くなる可能性がある | これらのエラーは、専門用語によって生じる可能性がある |

WER の構成要素 (挿入エラー、削除エラー、および置換エラーの数) を判断すると、モデルを向上させるために追加すべきデータの種類を判断できます。 [Custom Speech ポータル](https://speech.microsoft.com/customspeech)を使用して、ベースライン モデルの品質を確認します。 このポータルでは、挿入、置換、および削除のエラー率が WER 品質評価で結合されて報告されます。

## <a name="improve-model-recognition"></a>モデル認識を向上させる

[Custom Speech ポータル](https://speech.microsoft.com/customspeech)にトレーニング データを追加することで、認識エラーを減らすことができます。 

ソース マテリアルを定期的に追加して、カスタム モデルの保守を計画します。 カスタム モデルには、エンティティへの変更を把握するために追加のトレーニングが必要です。 たとえば、製品名、曲名、または新しいサービスの場所の更新が必要になる場合があります。

以下のセクションでは、追加の各種トレーニング データによってエラーが減少するしくみについて説明します。

### <a name="add-related-text-sentences"></a>関連テキスト文を追加する

新しいカスタム モデルをトレーニングするときは、まず、関連するテキストを追加して、ドメイン固有の単語や語句の認識を向上します。 関連テキスト文を使用すると、一般的な単語やドメイン固有の単語をコンテキストに表示することによって、その単語の誤認識に関連する置換エラーを主に減らすことができます。 ドメイン固有の単語は、一般的でない単語や造語である可能性がありますが、認識できるように明快に発音する必要があります。

> [!NOTE]
> 認識できない文字や単語などのノイズを含む関連テキスト文は避けてください。

### <a name="add-audio-with-human-labeled-transcripts"></a>"人間" とラベルが付いた文字起こしデータを含むオーディオを追加する

"人間" とラベルが付いた文字起こしデータを含むオーディオでは、オーディオがターゲットのユース ケースに由来する場合に、精度が最大限に向上します。 サンプルは、音声の完全な範囲をカバーする必要があります。 たとえば、小売店のコール センターでは、夏の月間には水着とサングラスに関する電話を最も多く受けます。 サンプルに、検出する音声の完全なスコープが含まれていることを確認します。

次の詳細を考慮してください。

* オーディオを使ったトレーニングは、人間にとっても音声が聞き取りにくい場合に最大のメリットが得られます。 ほとんどの場合、トレーニングは、単に関連するテキストを使用することから開始します。
* 米国英語など、最もよく使用されている言語のいずれかを使用している場合は、音声データでのトレーニングが不要になる可能性があります。 このような言語では、ほとんどのシナリオで基本モデルで既に非常に優れた認識結果が得られるため、関連するテキストでトレーニングするだけで十分であると考えられます。
* Custom Speech では、単語のコンテキストのみがキャプチャされて、挿入エラーや削除エラーではなく置換エラーを減らすことができます。
* 文字起こしエラーを含むサンプルは避けてください。ただし、音質の多様性は含めます。
* 問題のあるドメインに関連付けられていない文は避けてください。 関連のない文は、モデルに悪影響を及ぼす可能性があります。
* 文字起こしデータの品質が多様である場合は、特に優れた文 (キー フレーズを含む優れた文字起こしなど) を複製して重みを上げることができます。
* 音声サービスは、関連するテキストとして追加されたかのように、トランスクリプトを自動的に使用してドメイン固有の単語や語句の認識を改善します。
* トレーニング操作の完了には、数日かかる場合があります。 トレーニング速度を上げるには、トレーニング用の[専用ハードウェアがあるリージョン](custom-speech-overview.md#set-up-your-azure-account)に、音声サービスのサブスクリプションを作成する必要があります。

> [!NOTE]
> すべての基本モデルでオーディオのトレーニングがサポートされるわけではありません。 基本モデルでサポートされていない音声サービスは、トランスクリプトのテキストのみを使用し、オーディオを無視します。 オーディオ データを使用したトレーニングをサポートする基本モデルの一覧については、「[言語のサポート](language-support.md#speech-to-text)」を参照してください。 基本モデルでオーディオ データを使用したトレーニングがサポートされている場合でも、サービスによってオーディオの一部しか使用されないことがあります。 その場合も、すべてのトランスクリプトが使用されます。

> [!NOTE]
> トレーニングに使用する基本モデルを変更し、トレーニング データセットにオーディオが含まれる場合は、選択した新しい基本モデルが [オーディオ データを使用したトレーニングをサポート](language-support.md#speech-to-text)しているかどうかを "*常に*" 確認します。 以前使用した基本モデルでオーディオ データを使用したトレーニングがサポートされておらず、トレーニング データセットにオーディオが含まれる場合は、新しい基本モデルを使用したトレーニングの時間が **大幅に** 増加し、数時間から数日以上かかる可能性が大いにあります。 これは特に、音声サービスのサブスクリプションが、トレーニング用の [専用ハードウェアがあるリージョン](custom-speech-overview.md#set-up-your-azure-account)に **存在しない** 場合に当てはまります。
>
> 上の段落で説明されている問題が発生した場合、データセット内のオーディオの量を減らすか、完全に削除してテキストのみを残すことで、トレーニング時間を簡単に短縮できます。 音声サービスのサブスクリプションが、トレーニング用の [専用ハードウェアがあるリージョン](custom-speech-overview.md#set-up-your-azure-account)に **存在しない** 場合、後者のオプションを強くお勧めします。

### <a name="add-new-words-with-pronunciation"></a>発音を含む新しい単語を追加する

造語や高度に特殊化された単語は、独特の発音を持つ場合があります。 単語をより小さい単語に分割して発音できる場合は、これらの単語を認識できます。 たとえば、**Xbox** を認識するには、**エックス ボックス** と発音します。 この方法では全体的な精度が向上するわけではありませんが、これらのキー ワードの認識度を高めることができます。

> [!NOTE]
> この手法は、現時点では一部の言語でのみ使用できます。 詳細については、[音声テキスト変換の表](language-support.md)に示される発音のカスタマイズを参照してください。

## <a name="sources-by-scenario"></a>シナリオ別のソース

次の表に、音声認識のシナリオと、上記の 3 つのトレーニング コンテンツ カテゴリ内で考慮する必要があるソース マテリアルを示します。

| シナリオ | 関連テキスト文 | "オーディオ + 人間" とラベルが付いたトランスクリプト | 発音を含む新しい単語 |
|----------|------------------------|------------------------------|------------------------------|
| コール センター             | マーケティング ドキュメント、Web サイト、コール センター アクティビティに関連する製品レビュー | 人間によって文字起こしデータが作成されたコール センターのコール | あいまいな発音を持つ用語 (上記の「Xbox」を参照) |
| 音声アシスタント         | コマンドとエンティティのすべての組み合わせを使用して文を一覧表示する | コマンドを読み上げる音声をデバイスに録音し、テキストに変換する | 独特の発音を持つ名前 (映画、曲、製品) |
| ディクテーション               | 手書き入力 (インスタント メッセージや電子メールなど) | 上記と同様 | 上記と同様 |
| 動画の字幕 | TV 番組のスクリプト、映画、マーケティング コンテンツ、動画の要約 | 動画の正確な文字起こしデータ | 上記と同様 |

## <a name="next-steps"></a>次のステップ

* [モデルのトレーニングとデプロイ](how-to-custom-speech-train-model.md)

## <a name="additional-resources"></a>その他のリソース

* [データを準備してテストする](./how-to-custom-speech-test-and-train.md)
* [データを検査する](how-to-custom-speech-inspect-data.md)