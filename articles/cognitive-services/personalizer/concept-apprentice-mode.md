---
title: 見習いモード - Personalizer
description: ''
ms.topic: conceptual
ms.date: 05/01/2020
ms.openlocfilehash: 2697ab4b32edbd4841f2b11725fda46e90e7ae7e
ms.sourcegitcommit: bb0afd0df5563cc53f76a642fd8fc709e366568b
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 05/19/2020
ms.locfileid: "83599307"
---
# <a name="use-apprentice-mode-to-train-personalizer-without-affecting-your-existing-application"></a>見習いモードを使用して、既存のアプリケーションに影響を与えずに Personalizer をトレーニングする

**実際**の補強学習の性質により、Personalizer モデルは運用環境でのみトレーニングできます。 新しいユース ケースをデプロイする場合、モデルが十分にトレーニングされるまで時間がかかるため、Personalizer モデルは効率的には実行されません。  **見習いモード**は、このような状況を軽減し、開発者がコードを変更することなくモデルの自信を得ることができる学習動作です。

[!INCLUDE [Important Blue Box - Apprentice mode pricing tier](./includes/important-apprentice-mode.md)]

## <a name="what-is-apprentice-mode"></a>見習いモードとは

見習いがマスターから学習し、経験によってさらに向上するのと同じように、見習いモードは、既存のアプリケーション ロジックから取得された結果を観察することによって Personalizer が学習できるようにする "_動作_" です。

Personalizer は、アプリケーションと同じ出力を模倣することによってトレーニングを行います。 イベント フローが増えると、Personalizer は既存のロジックと結果に影響を与えることなく、既存のアプリケーションに "_追い付く_" ことができます。 Azure portal と API から使用できるメトリックは、モデルが学習するパフォーマンスを把握するのに役立ちます。

Personalizer が一定のレベルの理解を学習して獲得すると、開発者は動作を見習いモードからオンライン モードに変更できます。 その時点で、Personalizer は Rank API でのアクションに影響を与えるようになります。

## <a name="purpose-of-apprentice-mode"></a>見習いモードの目的

見習いモードを使用すると、Personalizer サービスとその機械学習機能が信頼できるようになり、オンライン トラフィックを危険にすることなく、学習可能な情報がサービスに送信されることの再保証が提供されます。

見習いモードを使用する主な理由は、次の 2 つです。

* **コールド スタート**の軽減: 見習いモードは、最良のアクションが返されず、75 - 85% 前後の十分なレベルの効果が得られない場合に、"新しい" モデルの学習時間を管理および評価するのに役立ちます。
* **アクションとコンテキストの特徴の検証**: アクションとコンテキストで送信される特徴は、少なすぎる、多すぎる、正しくない、詳細すぎるなど、理想的な有効度を達成するために Personalizer をトレーニングするのに不適切または不正確である場合があります。 特徴に関する問題を見つけて修正するには、[特徴評価](concept-feature-evaluation.md)を使用します。

## <a name="when-should-you-use-apprentice-mode"></a>どのようなときに見習いモードを使用する必要があるか

ユーザーのエクスペリエンスが Personalizer によって影響を受けないようにしながら、次のシナリオを通して有効性が向上するように Personalizer をトレーニングするには、見習いモードを使用します。

* 新しいユース ケースに Personalizer を実装しています。
* コンテキストまたはアクションで送信する特徴を大幅に変更しました。
* 報酬を計算するタイミングと方法を大幅に変更しました。

見習いモードは、報酬スコアに対する Personalizer の影響を測定する効果的な方法ではありません。 各 Rank の呼び出しに対して最適なアクションの選択での Personalizer の効果を測定するには、[オフライン評価](concepts-offline-evaluation.md)を使用します。

## <a name="who-should-use-apprentice-mode"></a>見習いモードを使用する必要があるユーザー

見習いモードは、開発者、データ サイエンティスト、およびビジネス上の意思決定者に役立ちます。

* **開発者**は、見習いモードを使用して、Rank API と Reward API がアプリケーションで正しく使用されていること、およびアプリケーションから Personalizer に送信される特徴にバグ、またはタイムスタンプや UserID 要素などの関連のない特徴が含まれないことを、確認することができます。

* **データ サイエンティスト**は、見習いモードを使用して、特徴が Personalizer モデルのトレーニングに効果的であること、および報酬の待機時間が長すぎたり短かすぎたりしないことを、検証できます。

* **ビジネス上の意思決定者**は、見習いモードを使用して、既存のビジネス ロジックと比較して Personalizer により結果 (つまり報酬) が向上する可能性を評価することができます。 これにより、実際の収益やユーザー満足度につながるユーザー エクスペリエンスに影響を与える決定を、情報に基づいて行うことができます。

## <a name="comparing-behaviors---apprentice-mode-and-online-mode"></a>動作の比較 - 見習いモードとオンライン モード

見習いモードでの学習は、次の点がオンライン モードと異なります。

|領域|見習いモード|オンライン モード|
|--|--|--|
|ユーザー エクスペリエンスへの影響|既存のユーザー動作を使用し、**既定のアクション**とそれによって獲得された報酬を(影響を与えるのではなく) 観察することによって、Personalizer をトレーニングできます。 これは、ユーザーのエクスペリエンスとビジネスの結果が影響を受けないことを意味します。|Rank の呼び出しから返された上位のアクションを表示して、ユーザーの動作に影響を与えます。|
|学習速度|Personalizer の学習速度は、オンライン モードで学習しているときより、見習いモードのときの方が遅くなります。 見習いモードでは、**既定のアクション**によって獲得される報酬を観察することによってのみ学習できます。そのため、探索を実行できないので、学習の速度が制限されます。|現在のモデルの活用と新しい傾向の調査の両方が可能であるため、より迅速に学習できます。|
|学習効果の "天井"|Personalizer は、基になるビジネス ロジックを近似でき、一致することはほとんどなく、超えることはありません (各 Rank 呼び出しの**既定のアクション**によって獲得される報酬合計)。|Personalizer はアプリケーションのベースラインを超える必要があり、時間が経って行き詰まったら、オフライン評価と特徴評価を実施して、モデルの改善を続ける必要があります。 |
|rewardActionId に対する Rank API の値|_rewardActionId_ は常に Rank 要求で送信する最初のアクションであるため、ユーザーのエクスペリエンスが影響を受けることはありません。 つまり、見習いモードの間は、Rank API はアプリケーションに対して目に見えることは何も行いません。 アプリケーションの Reward API は、モードが変わっても、Reward API の使用方法が変更されないようにする必要があります。|ユーザーのエクスペリエンスは、Personalizer によってアプリケーションに対して選択された _rewardActionId_ により変更されます。 |
|評価|Personalizer では、既定のビジネス ロジックによって得られる報酬の合計と、その時点でオンライン モードになっている場合に Personalizer が獲得する報酬の合計の比較が保持されています。 比較は、そのリソースの Azure portal で使用できます|[オフライン評価](concepts-offline-evaluation.md)を実行することによって、Personalizer の有効性を評価します。これにより、Personalizer によって達成された合計報酬と、アプリケーションのベースラインの予想される報酬を比較できます。|

見習いモードの有効性に関する注意事項:

* 見習いモードでの Personalizer の有効性は、アプリケーションのベースラインの 100% 近くを達成することはほとんどなく、それを超えることはありません。
* ベスト プラクティスは、100% の達成を試みないようにすることです。ただし、ユース ケースによっては、75 から 85% の範囲を対象にする必要があります。

## <a name="using-apprentice-mode-to-train-with-historical-data"></a>見習いモードを使用して履歴データでトレーニングを行う

大量の履歴データがあり、Personalizer のトレーニングに使用したい場合は、見習いモードを使用し、Personalizer を通してデータを再生できます。

見習いモードで Personalizer を設定し、履歴データからのアクションとコンテキストの特徴で Rank を呼び出すスクリプトを作成します。 このデータのレコードの計算に基づいて、Reward API を呼び出します。 何らかの結果を得るには約 5 万件の履歴イベントが必要ですが、結果の信頼性を高めるために 50 万件が推奨されます。

履歴データからトレーニングするときは、送信されるデータ (コンテキストとアクションの特徴、Rank 要求で使用される JSON のレイアウト、このトレーニング データ セットでの報酬の計算) を、既存のアプリケーションから利用できるデータ (特徴および特典の計算) と一致させることをお勧めします。

オフライン データと事後データは、より不完全で、形式の不備や相違が多くなる傾向があります。 履歴データからのトレーニングを行うことは可能ですが、その結果は決定的でなく、Personalizer の学習の結果を予測するのに適していない可能性があります。過去のデータと既存のアプリケーションで特徴が異なる場合は特にそうです。

一般に、Personalizer では、履歴データを使用してトレーニングするより、見習いモードに動作を変更し、既存のアプリケーションから学習した方が、効果的なモデルを作成するためのより効率的なパスになり、作業量、Data Engineering、クリーンアップ作業が少なくなります。

## <a name="using-apprentice-mode-versus-ab-tests"></a>見習いモードの使用と A/B テストの使用

検証が済み、オンライン モードで学習している場合にのみ、Personalizer の処理の A/B テストを行うと役に立ちます。 見習いモードでは、**既定のアクション**のみが使用されます。これは、すべてのユーザーに対してコントロール エクスペリエンスが効果的に表示されることを意味します。

Personalizer が "_処理_" だけであっても、データの検証が Personalizer のトレーニングに対して適切であるときは、同じ課題が存在します。 100% のトラフィックを使用し、すべてのユーザーがコントロール (影響を受けない) エクスペリエンスを取得するようにして、代わりに見習いモードを使用できます。

Personalizer を使用してオンラインで学習するユース ケースができたら、A/B 実験を使用して、報酬に使用されるシグナルより複雑な可能性がある結果の制御されたコーホートと科学的な比較を行うことができます。 A/B テストで回答できる質問の例を次に示します: `In a retail website, Personalizer optimizes a layout and gets more users to _check out_ earlier, but does this reduce total revenue per transaction?`

## <a name="next-steps"></a>次のステップ

* [アクティブなイベントと非アクティブなイベント](concept-active-inactive-events.md)について学習します