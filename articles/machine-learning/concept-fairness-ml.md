---
title: 機械学習モデルでの不公平性を軽減する (プレビュー)
titleSuffix: Azure Machine Learning
description: 機械学習モデルでの公平性と、Fairlearn Python パッケージを使用してより公平なモデルを構築する方法について説明します。
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: luquinta
author: luisquintanilla
ms.date: 01/26/2021
ms.custom: responsible-ml
ms.openlocfilehash: 276c91ad7fb5b09dbe18d989741f0f54b8b0eb09
ms.sourcegitcommit: 910a1a38711966cb171050db245fc3b22abc8c5f
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 03/20/2021
ms.locfileid: "101659683"
---
# <a name="mitigate-unfairness-in-machine-learning-models-preview"></a>機械学習モデルでの不公平性を軽減する (プレビュー)

機械学習での公平性と、[Fairlearn](https://fairlearn.github.io/) オープンソース Python パッケージを使用して、機械学習モデルでの不公平性の問題を軽減する方法について説明します。 機械学習モデルを構築するとき、公平性の問題を理解し公平性を評価するよう努力しなかった場合、不公平な結果を生成するモデルが構築されることがあります。

以下に示す Fairlearn オープンソース パッケージの[ユーザー ガイド](https://fairlearn.github.io/main/user_guide/index.html)の要約には、これを使用して、構築する AI システムの公平性を評価する方法が記載されています。  また、Fairlearn オープンソース パッケージには、観察される公平性の問題を軽減したり、削減したりするのに役立つオプションも用意されています。  Azure Machine Learning でのトレーニング中に AI システムの公平性の評価を有効にするには、[操作方法](how-to-machine-learning-fairness-aml.md)と[サンプル ノートブック](https://github.com/Azure/MachineLearningNotebooks/tree/master/contrib/fairness)に関するページを参照してください。


## <a name="what-is-fairness-in-machine-learning-models"></a>機械学習システムでの公平性とは

>[!NOTE]
> 公平性は、社会技術に関する課題です。 正当性や適正手続きなど、公平性の多くの側面は、定量的な公平性のメトリックではキャプチャされません。 また、多くの定量的な公平性のメトリックをすべて同時に満たすことはできません。 Fairlearn オープンソース パッケージの目的は、人間がさまざまな影響と軽減の戦略を評価できるようにすることです。 最終的には、人工知能および機械学習モデルを構築する人間のユーザーが、シナリオに適した妥協点を見つけることになります。

人工知能および機械学習システムでは、不公平な動作が示されることがあります。 不公平な動作を定義する方法の 1 つとして、人への危害や影響があります。 AI システムでは、さまざまな種類の危害が発生する可能性があります。 詳細については、[Kate Crawford による NeurIPS 2017 基調講演](https://www.youtube.com/watch?v=fMym_BKWQzk)を参照してください。

AI によって発生する 2 つの一般的な種類の危害を次に示します。

- 割り当ての害: AI システムによって、特定のグループの機会、リソース、または情報が増減されます。 たとえば、雇用、入学許可、融資などで、モデルにより、特定のグループの人が、他のグループより、適切な候補をうまく選択される場合があります。

- サービス品質の害: AI システムによる対応のよさが、ユーザーのグループによって異なります。 たとえば、音声認識システムでは、女性に対する対応が男性より悪くなる場合があります。

AI システムの不公平な動作を減らすには、これらの害を評価し、軽減する必要があります。

## <a name="fairness-assessment-and-mitigation-with-fairlearn"></a>Fairlearn を使用した公平性の評価と軽減

Fairlearn はオープンソースの Python パッケージです。機械学習システムの開発者は、これを使用して、システムの公平性を評価し、不公平性を軽減できます。

Fairlearn オープンソース パッケージには、次の 2 つのコンポーネントがあります。

- 評価ダッシュボード: モデルの予測がさまざまなグループに与える影響を評価するための Jupyter Notebook ウィジェット。 また、公平性とパフォーマンスのメトリックを使用して、複数のモデルを比較することもできます。
- 軽減アルゴリズム: 二項分類と回帰を使用して不公平性を軽減するためのアルゴリズムのセット。

これらのコンポーネントの組み合わせにより、データ サイエンティストやビジネス リーダーは、公平性とパフォーマンスの間のさまざまなトレードオフを確認し、ニーズに最も合った軽減策を選択できます。

## <a name="assess-fairness-in-machine-learning-models"></a>機械学習モデルでの公平性を評価する

Fairlearn オープンソース パッケージでは、公平性は、次のことを質問する **グループ公平性** と呼ばれるアプローチによって概念化されます。損害が発生するリスクがあるのはどのグループか。 関連グループ (部分母集団とも呼ばれます) は、**微妙な特徴** または微妙な属性によって定義されます。 微妙な特徴は、ベクトルまたは `sensitive_features` と呼ばれるマトリックスとして Fairlearn  オープンソース パッケージの推定器に渡されます。 この用語は、グループの公平性を評価するときに、システム デザイナーがこれらの特徴に敏感でなければならないことを示しています。 

注意すべき点は、これらの特徴にプライベート データによるプライバシーへの影響があるかどうか、ということです。 ただし、"微妙な" という言葉は、予測を行うときにこれらの特徴を使用してはならないという意味ではありません。

>[!NOTE]
> 公平性の評価は純粋に技術的な行為ではありません。  Fairlearn オープンソース パッケージはモデルの公平性を評価するのに役立ちますが、ユーザーの代わりに評価を実行しません。  Fairlearn オープンソース パッケージは、公平性を評価するための定量的なメトリックを特定するのに役立ちますが、開発者は独自のモデルの公平性を評価するために、定性分析を実行する必要もあります。  上記の微妙な特徴は、このような定性分析の一例です。     

評価段階では、公平性は不均衡メトリックによって定量化されます。 **不均衡メトリック** では、比率または相違として、異なるグループの間でのモデルの動作を評価および比較できます。 Fairlearn オープンソース パッケージでは、不均衡メトリックの 2 つのクラスがサポートされています。


- モデルのパフォーマンスにおける不均衡: これらのメトリックのセットでは、異なるサブグループ間での、選択されたパフォーマンス メトリックの値の不均衡 (差異) が計算されます。 次に例をいくつか示します。

  - 正解率の不均衡
  - エラー率の不均衡
  - 精度の不均衡
  - リコールの不均衡
  - MAE の不均衡
  - その他多数

- 選択率における不均衡: このメトリックには、異なるサブグループ間での選択率の差が含まれます。 この例としては、ローン承認率の不均衡があります。 選択率とは、各クラスで 1 として分類されるデータポイントの割合 (二項分類)、または予測値の分散 (回帰) を意味します。

## <a name="mitigate-unfairness-in-machine-learning-models"></a>機械学習モデルでの不公平性を軽減する

### <a name="parity-constraints"></a>不均衡の制約

Fairlearn オープンソース パッケージには、さまざまな不公平性軽減アルゴリズムが含まれています。 これらのアルゴリズムでは、**不均衡の制約** または条件と呼ばれる、予測の動作に対する一連の制約がサポートされています。 不均衡の制約では、予測動作の一部の側面が、微妙な特徴で定義されるグループ (異なる人種など) の間で同等であることが要求されます。 Fairlearn オープンソース パッケージの軽減アルゴリズムでは、このような不均衡の制約を使用して、監視対象の公平性の問題が軽減されます。

>[!NOTE]
> モデルの公平性を軽減することは、不公平性を減らすことを意味しますが、この技術的な軽減策では、このような不公平性を完全に排除することはできません。  Fairlearn オープンソース パッケージの不公平性の軽減アルゴリズムでは、機械学習モデルの不公平性を削減するのに役立つ推奨される軽減戦略を提供できますが、これらは不公平性を完全に排除するソリューションではありません。  個々の特定の開発者機械学習モデルには、それぞれ考慮すべき不均衡の制約または条件が他にある場合もあります。 Azure Machine Learning を使用する開発者は、その軽減策によって、対象となる機械学習モデルの使用およびデプロイでの不公平性が十分に排除されるかどうかを自分で判断する必要があります。  

Fairlearn オープンソース パッケージでは、次の種類の不均衡の制約がサポートされています。 

|不均衡の制約  | 目的  |機械学習タスク  |
|---------|---------|---------|
|人口統計の不均衡     |  割り当ての害を軽減する | 二項分類、回帰 |
|均等な確率  | 割り当てとサービス品質の害を診断する | 二項分類        |
|機会均等 | 割り当てとサービス品質の害を診断する | 二項分類        |
|境界グループの損失     |  サービス品質の害を軽減する | 回帰 |



### <a name="mitigation-algorithms"></a>軽減アルゴリズム

Fairlearn オープンソース パッケージでは、後処理と削減の不公平性軽減アルゴリズムが提供されています。

- 削減: これらのアルゴリズムでは、標準のブラックボックス機械学習推定器 (LightGBM モデルなど) が利用され、一連の再重み付けされたトレーニング データセットを使用して、再トレーニングされたモデルのセットが生成されます。 たとえば、特定の性別の応募者について、重みを加減してモデルが再トレーニングされ、異なる性別グループ間での格差が削減されます。 その後、ユーザーは、精度 (または他のパフォーマンス メトリック) と不均衡の間で最適なトレードオフを提供するモデルを選択できます。これは通常、ビジネス ルールとコストの計算に基づいている必要があります。  
- 後処理: これらのアルゴリズムは、既存の分類子と微妙な特徴を入力として受け取ります。 その後、分類子の予測の変換を導出して、指定された公平性の制約を適用します。 しきい値の最適化の最大の利点は、モデルを再トレーニングする必要がないことによる、シンプルさと柔軟性です。 

| アルゴリズム | 説明 | 機械学習タスク | 微妙な特徴 | サポートされる不均衡の制約 | アルゴリズムの種類 |
| --- | --- | --- | --- | --- | --- |
| `ExponentiatedGradient` | 「[公平な分類のための削減アプローチ](https://arxiv.org/abs/1803.02453)」で説明されている公平な分類のためのブラックボックス アプローチ | 二項分類 | Categorical | [人口統計の不均衡](#parity-constraints)、[均等な確率](#parity-constraints) | 削減 |
| `GridSearch` | 「[公平な分類のための削減アプローチ](https://arxiv.org/abs/1803.02453)」で説明されているブラックボックス アプローチ| 二項分類 | Binary | [人口統計の不均衡](#parity-constraints)、[均等な確率](#parity-constraints) | 削減 |
| `GridSearch` | 境界グループの損失に対するアルゴリズムを使用する公正回帰のグリッド検索バリエーションを実装するブラックボックス アプローチ (「[公正回帰: 定量的な定義と削減に基づくアルゴリズム](https://arxiv.org/abs/1905.12843)」) | 回帰 | Binary | [境界グループの損失](#parity-constraints) | 削減 |
| `ThresholdOptimizer` | ホワイトペーパー「[教師あり学習での機会の均等性](https://arxiv.org/abs/1610.02413)」に基づく後処理アルゴリズム。 この手法では、既存の分類子と微妙な特徴を入力として受け取り、指定された不均衡の制約を適用するために、分類子の予測の単調な変換が導出されます。 | 二項分類 | Categorical | [人口統計の不均衡](#parity-constraints)、[均等な確率](#parity-constraints) | 後処理 |

## <a name="next-steps"></a>次のステップ

- Fairlearn の [GitHub](https://github.com/fairlearn/fairlearn/)、[ユーザー ガイド](https://fairlearn.github.io/main/user_guide/index.html)、[例](https://fairlearn.github.io/main/auto_examples/index.html)、および[サンプル ノートブック](https://github.com/fairlearn/fairlearn/tree/master/notebooks)を確認することによってさまざまなコンポーネントの使用方法を学習します。
- Azure Machine Learning の機械学習モデルの公平性評価を実現する[方法](how-to-machine-learning-fairness-aml.md)について学習します。
- Azure Machine Learning でのその他の公平性の評価シナリオについては、[サンプル ノートブック](https://github.com/Azure/MachineLearningNotebooks/tree/master/contrib/fairness)を参照してください。 
